---
title: "Monte Carlo Simulation"
subtitle: "Lecture 11"
author: "Vivek Srikrishnan"
course: "BEE 4750"
institution: "Cornell University"
date: "September 13, 2023"
format:
    revealjs:
        slide-number: c/t
        show-slide-number: all
        center-title-slide: true
        width: 1280
        height: 720
        transition: none
        toc: true
        toc-depth: 1
        toc-title: "Overview"
        history: false
        link-external-newwindow: true
        theme: ../sass/slides.scss
        footer: "[BEE 4750, Environmental Systems Analysis](https://viveks.me/environmental-systems-analysis)"
        template-partials:
            - title-slide.html
        menu:
            numbers: true
        html-math-method: mathjax
        include-in-header: mathjax-config.html
        date-format: long
        highlight-style: tango
        code-line-numbers: false

execute:
    freeze: auto
---

```{julia}
#| echo: false
#| output: false

import Pkg
Pkg.activate(".")
Pkg.instantiate()
```

```{julia}
#| echo: false
#| output: false

using Measures
using Random
using Distributions
using Plots
using StatsPlots
using Animations

Random.seed!(1)
```
# Review and Questions

## Last Class

- Used dissolved oxygen model to simulate dynamics.
- Competing consumption/aeration processes result in "sag" curve downstream of waste release.
- Results in difficulty assigning responsibility when dealing with multiple releases.
- When simulating with multiple boxes/initial conditions, remember to track all relevant state variables and update!

## Questions?

{{< include _poll-prompt.qmd >}}

# Probability

## Systems and Uncertainty

:::: {.columns}
::: {.column width=50%}
Deterministic systems models can be subject to uncertainties due to the separation between the "internals" of the system and the "external" environment.
:::

::: {.column width=50%}
![Conceptual Schematic of a Systems Model](images/system-conceptual.svg)
:::
::::

## Definition of Probability

We often deal with uncertainty using probabilities.

Probability is:

- Long-run frequency of an event (**frequentist**)
- Degree of belief that a proposition is true (**Bayesian**)

The distinction isn't essential for our purposes, but might be in the future...

## Probability Distributions

The likelihood of possible values of an unknown quantity are often represented as a probability distribution.

Probability distributions associate a probability to every event under consideration and have to follow certain rules (for example, total probability = 1).

## Selecting a Distribution

The specification of distributions can *strongly* influence the analysis. 

## Selecting a Distribution

A distribution implicitly answers questions like:

- What is the most probable event? How much more likely is it than the others?
- Are larger or smaller events more, less, or equally probable?
- How probable are extreme events?
- Are different events correlated, or are they independent?

## Key Features of Probability Distributions

- Mean/Mode (what events are "typical")
- Skew (are larger or smaller events more or equally probable)
- Variance (how spread out is the distribution around the mode)
- Tail Probabilities (how probable are extreme events)

## Probability Distribution Tails

:::: {.columns}
::: {.column width=50%}
The tails of distributions represent the probability of high-impact outcomes.

**Key consideration**: Small changes to these (low) probabilities can greatly influence risk.
:::

::: {.column width=50%}
```{julia}
x = range(-5, 10; length = 100)
plot(x, pdf.(Cauchy(), x), linewidth=3, linecolor = :red, linestyle = :dash, yaxis = false, yticks = false, grid = false, label = "Cauchy Distribution", size=(500, 400), guidefontsize=16, legendfontsize=14, tickfontsize=16)
plot!(x, pdf.(Normal(), x), linewidth=3, linecolor = :blue, label = "Normal Distribution")
plot!(x[x .> 1.75], pdf.(Cauchy(), x[x .> 1.75]), fillrange = pdf.(Normal(), x[x .> 1.75]), fillcolor = :red, fillalpha = 0.2, label = false, linecolor= false)
xlabel!("Value")
```
:::
::::

# Monte Carlo

## Stochastic Simulation

**Monte Carlo simulation**: Propagating random samples through a model.

```{dot}
//| fig-width: 100%
digraph G {
    graph [
        rankdir=LR
        layout=dot
    ]
    node [
        fontname = "IBM Plex Sans, sans-serif"
        fontsize=25
    ]
    edge [
        arrowsize=0.75
        labeldistance=3
        penwidth=3
        fontname = "IBM Plex Sans, sans-serif"
        fontsize=25
        style=dashed
        color="#b31b1b"
        fontcolor="#b31b1b"
    ]
    a [label="Probability\n Distribution"]
    b [label = "Random\n Samples"]
    c [label="Model"]
    d [label="Outputs"]

    a -> b [
        label="Sample"
    ]
    b -> c [
        label="Input"
    ]
    c -> d [
        label="Simulate"
    ]
}

```

## Goals of Monte Carlo

Monte Carlo is a broad method, which can be used to:

1. Obtain probability distributions of outputs;
2. Estimate deterministic quantities (***Monte Carlo estimation***).

## Monte Carlo Estimation

Monte Carlo estimation involves framing the quantity of interest as a summary statistic (such as an expected value).

## MC Example: Finding $\pi$

:::: {.columns}
::: {.column width=50%}
Finding $\pi$ by sampling random values from the unit square and computing the fraction in the unit circle. This is an example of **Monte Carlo integration**.

$$\frac{\text{Area of Circle}}{\text{Area of Square}} = \frac{\pi}{4}$$
:::
::: {.column width=50%}

```{julia}
#| echo: false
#| fig-height: 100%
function circleShape(r)
    θ = LinRange(0, 2 * π, 500)
    r * sin.(θ), r * cos.(θ)
end

nsamp = 3000
unif = Uniform(-1, 1)
x = rand(unif, (nsamp, 2))
l = mapslices(v -> sum(v.^2), x, dims=2)
in_circ = l .< 1
pi_est = [4 * mean(in_circ[1:i]) for i in 1:nsamp]

plt1 = plot(
    1,
    xlim = (-1, 1),
    ylim = (-1, 1),
    legend = false,
    markersize = 4,
    framestyle = :origin,
    tickfontsize=16,
    grid=:false
    )
plt2 = plot(
    1,
    xlim = (1, nsamp),
    ylim = (3, 3.5),
    legend = :false,
    linewidth=3, 
    color=:black,
    tickfontsize=16,
    guidefontsize=16,
    xlabel="Iteration",
    ylabel="Estimate",
    right_margin=5mm
)
hline!(plt2, [π], color=:red, linestyle=:dash)
plt = plot(plt1, plt2, layout=grid(2, 1, heights=[2/3, 1/3]), size=(600, 600))

plot!(plt, circleShape(1), linecolor=:blue, lw=1, aspectratio=1, subplot=1)


mc_anim = @animate for i = 1:nsamp
    if l[i] < 1
        scatter!(plt[1], Tuple(x[i, :]), color=:blue, markershape=:x, subplot=1)
    else
        scatter!(plt[1], Tuple(x[i, :]), color=:red, markershape=:x, subplot=1)
    end
    push!(plt, 2, i, pi_est[i])
end every 100

gif(mc_anim, "images/mc_pi.gif", fps=3)
```
:::
::::

## MC Example: Dice

:::: {.columns}
::: {.column width=50%}
What is the probability of rolling 4 dice for a total of 19?

Can simulate dice rolls and find the frequency of 19s among the samples.
:::
::: {.column width=50%}

```{julia}
#| echo: false
#| fig-width: 100%

function dice_roll_repeated(n_trials, n_dice)
    dice_dist = DiscreteUniform(1, 6) 
	roll_results = zeros(n_trials)
	for i=1:n_trials
		roll_results[i] = sum(rand(dice_dist, n_dice))
	end
	return roll_results
end

nsamp = 10000
# roll four dice 10000 times
rolls = dice_roll_repeated(nsamp, 4) 

# calculate probability of 19
sum(rolls .== 19) / length(rolls)

# initialize storage for frequencies by sample length
avg_freq = zeros(length(rolls)) 
# compute average frequencies of 19
avg_freq[1] = (rolls[1] == 19)
count = 1
for i=2:length(rolls)
    avg_freq[i] = (avg_freq[i-1] * (i-1) + (rolls[i] == 19)) / i
end

plt = plot(
    1,
    xlim = (1, nsamp),
    ylim = (0, 0.1),
    legend = :false,
    tickfontsize=16,
    guidefontsize=16,
    xlabel="Iteration",
    ylabel="Estimate",
    right_margin=8mm,
    color=:black,
    linewidth=3,
    size=(600, 400)
)
hline!(plt, [0.0432], color="red", 
    linestyle=:dash) 

mc_anim = @animate for i = 1:nsamp
    push!(plt, 1, i, avg_freq[i])
end every 100

gif(mc_anim, "images/mc_dice.gif", fps=10)

```
:::
::::

## Monte Carlo Estimation

This type of estimation can be repeated with any simulation model that has a stochastic component.

For example, consider our dissolved oxygen model. Suppose that we have a probability distribution for the inflow DO.

**How could we compute the probability of DO falling below the regulatory standard somewhere downstream?**

## Why Monte Carlo Works

We can formalize this common use of Monte Carlo estimation as the computation of the expected value of a random quantity $Y$, $\mu = \mathbb{E}[Y]$.

To do this, generate $n$ independent and identically distributed values $Y_1, \ldots, Y_n$.  Then the sample estimate is

$$\tilde{\mu} = \frac{1}{n}\sum_{i=1}^n Y_i$$

## Monte Carlo (Formally)

More generally, we want to compute some quantity $Y=f(X)$, where $X$ is distributed according to some probability distribution $p(x)$ and $f(x)$ is a real-valued function over a domain $D$.

Then
$$\mu = \mathbb{E}(Y) = \int_D f(x)p(x)dx.$$

## The Law of Large Numbers

If 

(1) $Y$ is a random variable and its expectation exists and 

(2) $Y_1, \ldots, Y_n$ are independently and identically distributed

Then by the **weak law of large numbers**:

$$\lim_{n \to \infty} \mathbb{P}\left(\left|\tilde{\mu}_n - \mu\right| \leq \varepsilon \right) = 1$$

## The Law of Large Numbers

In other words, *eventually* we will get within an arbitrary error of the true expectation. But how large is large enough?

## Monte Carlo Sample Mean

Notice that the sample mean $\tilde{\mu}_n$ is itself a random variable.

With some assumptions (the mean of $Y$ exists and $Y$ has finite variance), the expected Monte Carlo sample mean $\mathbb{E}[\tilde{\mu}_n]$ is

$$\frac{1}{n}\sum_{i=1}^n \mathbb{E}[Y_i] = \frac{1}{n} n \mu = \mu$$

This means that the Monte Carlo estimate is an *unbiased* estimate of the mean.

## Monte Carlo Error

We'd like to know more about the error of this estimate for a given sample size. The variance of this estimator is

$$\tilde{\sigma}_n^2 = \text{Var}\left(\tilde{\mu}_n\right) = \mathbb{E}\left((\tilde{\mu}_n - \mu)^2\right) = \frac{\sigma_Y^2}{n}$$

So as $n$ increases, the *standard error* decreases:

$$\tilde{\sigma}_n = \frac{\sigma_Y}{\sqrt{n}}$$

## Monte Carlo Error

In other words, if we want to decrease the Monte Carlo error by 10x, we need 100x additional samples. **This is not an ideal method for high levels of accuracy.** 

::: {.fragment .fade-in}
::: {.quote}
> Monte Carlo is an extremely bad method. It should only be used when all alternative methods are worse.

::: {.cite}
--- Sokal, *Monte Carlo Methods in Statistical Mechanics*, 1996
:::
:::
:::

## Monte Carlo Confidence Intervals

This error estimate lets us compute confidence intervals for the MC estimate (details on [the website](https://viveks.me/environmental-systems-analysis/supplemental/mc-confidence.qmd)).

**Basic Idea**: The *Central Limit Theorem* says that with enough samples, the errors are normally distributed:

$$\left\|\tilde{\mu}_n - \mu\right\| \to \mathcal{N}\left(0, \frac{\sigma_Y^2}{n}\right)$$

## Monte Carlo Confidence Intervals

This means that we can construct confidence intervals using the inverse cumulative distribution function for the normal distribution.

The $\alpha$-confidence interval is:
$$\tilde{\mu}_n \pm \Phi^{-1}\left(1 - \frac{\alpha}{2}\right) \frac{\sigma_Y}{\sqrt{n}}$$

For example, the 95% confidence interval is $\tilde{\mu}_n \pm 1.96 \sqrt{\sigma_Y}{\sqrt{n}}$.

## Implications of Monte Carlo Error

Converging at a rate of $1/\sqrt{n}$ is not great. But:

- All models are wrong, and so there always exists some irreducible model error. 
- We often need a lot of simulations. Do we have enough computational power?

## When To Use Monte Carlo?

If you can compute your answers analytically, you probably should. 

**But for real problems, we often can't**, and problems that are analytically tractable are often so stylized that we want to test them across a variety of uncertainties to make sure we didn't over-simplify.
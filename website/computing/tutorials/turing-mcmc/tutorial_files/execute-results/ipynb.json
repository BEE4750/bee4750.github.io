{
  "hash": "ed3efd2fa4f582177b0234e611f1ca7a",
  "result": {
    "markdown": "---\ntitle: 'Tutorial: Markov Chain Monte Carlo With Turing.jl'\nexecute:\n  error: true\n  warning: true\nformat:\n  ipynb: default\nformat-links:\n  - ipynb\n---\n\n\n\n## Overview\n\nThis tutorial will give some examples of using `Turing.jl` and Markov Chain Monte Carlo to sample from posterior distributions.\n\n## Setup\n\n::: {#25cb4a58 .cell execution_count=2}\n``` {.julia .cell-code}\nusing Turing\nusing Distributions\nusing Plots\nusing LaTeXStrings\nusing StatsPlots\nusing StatsBase\nusing Optim\nusing Random\nusing DataFrames\nusing Downloads\nusing CSV\n```\n:::\n\n\nAs this tutorial involves random number generation, we will set a random seed to ensure reproducibility.\n\n::: {#fb147e2b .cell execution_count=3}\n``` {.julia .cell-code}\nRandom.seed!(1);\n```\n:::\n\n\n## Fitting A Linear Regression Model\n\nLet's start with a simple example: fitting a linear regression model to simulated data.\n\n::: {.callout-tip}\n## Positive Control Tests\n\nSimulating data with a known data-generating process and then trying to obtain the parameters for that process is an important step in any workflow.\n:::\n\n### Simulating Data\n\nThe data-generating process for this example will be:\n$$\n\\begin{gather}\ny = 5 + 4x + \\varepsilon \\\\\n\\varepsilon \\sim \\text{Normal}(0, 10),\n\\end{gather}\n$$\nwhere $\\varepsilon$ is so-called \"white noise\", which adds stochasticity to the data set. The generated dataset is shown in @fig-scatter-regression.\n\n::: {#fig-scatter-regression .cell .column-margin execution_count=4}\n``` {.julia .cell-code}\n# create trend for data\nx = rand(Uniform(0, 20), 10)\ny = 5 .+ 4 * x\n# sample and add noise\nε = rand(Normal(0, 10), 10)\ny .+= ε\n\nscatter(x, y, label=\"Data\", xlabel=L\"$x$\", ylabel=L\"$y$\", markersize=10, tickfontsize=14, guidefontsize=16, legendfontsize=16)\n```\n\n::: {.cell-output .cell-output-display execution_count=73}\n![Scatterplot of our generated data.](tutorial_files/figure-ipynb/fig-scatter-regression-output-1.png){#fig-scatter-regression}\n:::\n:::\n\n\n### Model Specification\n\nThe statistical model for a standard linear regression problem is\n$$\n\\begin{gather}\ny = a + bx + \\varepsilon \\\\\n\\varepsilon \\sim \\text{Normal}(0, \\sigma^2).\n\\end{gather}\n$$\n\nRearranging, we can rewrite the likelihood function as:\n$$y \\sim \\text{Normal}(\\mu, \\sigma^2),$$\nwhere $\\mu = a + bx$. This means that we have three parameters to fit: $a$, $b$, and $\\sigma^2$.\n\nNext, we need to select priors on our parameters. We'll use relatively generic distributions to avoid using the information we have (since we generated the data ourselves), but in practice, we'd want to use any relevant information that we had from our knowledge of the problem. Let's use relatively diffuse normal distributions for the trend parameters $a$ and $b$ and a half-normal distribution (a normal distribution truncated at 0, to only allow positive values) for the variance $\\sigma^2$, as recommended by [Gelman (2006)](https://doi.org/10.1214/06-BA117A).\n\n$$\n\\begin{gather}\na \\sim \\text{Normal(0, 10)} \\\\\nb \\sim \\text{Normal(0, 10)} \\\\\n\\sigma^2 \\sim \\text{Half-Normal}(0, 25)\n\\end{gather}\n$$\n\n### Using Turing\n\n#### Coding the Model\n\n`Turing.jl` uses the `@model` macro to specify the model function. We'll follow the setup in the [Turing documentation](https://turinglang.org/dev/tutorials/05-linear-regression).\n\nTo specify distributions on parameters (and the data, which can be thought of as uncertain parameters in Bayesian statistics), use a tilde `~`, and use equals `=` for transformations (which we don't have in this case).\n\n::: {#9640c548 .cell execution_count=5}\n``` {.julia .cell-code}\n@model function linear_regression(x, y)\n    # set priors\n    σ² ~ truncated(Normal(0, 25); lower=0)\n    a ~ Normal(0, 10)\n    b ~ Normal(0, 10)\n\n    # we can specify the likelihood with a loop, as our data is i.i.d.\n    # we could also rewrite this using linear algebra, which might be more efficient for large and/or complex models or datasets, but this will be more readable in this simple case.\n    for i = 1:length(y)\n        # compute the mean value for the data point\n        μ = a + b * x[i]\n        y[i] ~ Normal(μ, σ²)\n    end\nend\n```\n\n::: {.cell-output .cell-output-display execution_count=74}\n```\nlinear_regression (generic function with 2 methods)\n```\n:::\n:::\n\n\n#### Fitting The Model\n\nNow we can call the sampler to draw from the posterior. We'll use the [No-U-Turn sampler](http://www.stat.columbia.edu/~gelman/research/published/nuts.pdf), which is a Hamiltonian Monte Carlo algorithm (a different category of MCMC sampler than the Metropolis-Hastings algorithm discussed in class). We'll also use 4 chains so we can test that the chains are well-mixed, and each chain will be run for 5,000 iterations^[Hamiltonian Monte Carlo samplers often need to be run for fewer iterations than Metropolis-Hastings samplers, as the exploratory step uses information about the gradient of the statistical model, versus the random walk of Metropolis-Hastings. The disadvantage is that this gradient information must be available, which is not always the case for simulation models.]\n\n::: {#b78ebd39 .cell execution_count=6}\n``` {.julia .cell-code}\nchain = let\n    model = linear_regression(x, y) # create the model object with the data\n    sampler = NUTS()\n    n_per_chain = 5000\n    nchains = 4\n    # call the sampler and drop the \"burn-in/warm-up\" portion\n    sample(model, sampler, MCMCThreads(), n_per_chain, nchains, drop_warmup=true);\nend\n\n@show chain\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n┌ Warning: Only a single thread available: MCMC chains are not sampled in parallel\n└ @ AbstractMCMC ~/.julia/packages/AbstractMCMC/F9Hbk/src/sample.jl:296\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n┌ Info: Found initial step size\n└   ϵ = 0.00625\n┌ Info: Found initial step size\n└   ϵ = 0.05\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n┌ Info: Found initial step size\n└   ϵ = 0.003125\n\r┌ Info: Found initial step size\n└   ϵ = 0.0125\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nSampling (1 threads):  50%|██████████████▌              |  ETA: 0:00:00\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n\rSampling (1 threads): 100%|█████████████████████████████| Time: 0:00:00\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nchain = MCMC chain (5000×15×4 Array{Float64, 3})\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=75}\n\n::: {.ansi-escaped-output}\n```{=html}\n<pre>Chains MCMC chain (5000×15×4 Array{Float64, 3}):\nIterations        = 1001:1:6000\nNumber of chains  = 4\nSamples per chain = 5000\nWall duration     = 2.39 seconds\nCompute duration  = 2.2 seconds\nparameters        = σ², a, b\ninternals         = lp, n_steps, is_accept, acceptance_rate, log_density, hamiltonian_energy, hamiltonian_energy_error, max_hamiltonian_energy_error, tree_depth, numerical_error, step_size, nom_step_size\nSummary Statistics\n <span class=\"ansi-bold\"> parameters </span> <span class=\"ansi-bold\">    mean </span> <span class=\"ansi-bold\">     std </span> <span class=\"ansi-bold\"> naive_se </span> <span class=\"ansi-bold\">    mcse </span> <span class=\"ansi-bold\">       ess </span> <span class=\"ansi-bold\">    rhat </span> <span class=\"ansi-bold\"> </span> ⋯\n <span class=\"ansi-bright-black-fg\">     Symbol </span> <span class=\"ansi-bright-black-fg\"> Float64 </span> <span class=\"ansi-bright-black-fg\"> Float64 </span> <span class=\"ansi-bright-black-fg\">  Float64 </span> <span class=\"ansi-bright-black-fg\"> Float64 </span> <span class=\"ansi-bright-black-fg\">   Float64 </span> <span class=\"ansi-bright-black-fg\"> Float64 </span> <span class=\"ansi-bright-black-fg\"> </span> ⋯\n          σ²    9.9355    2.8951     0.0205    0.0329   8133.9030    1.0008    ⋯\n           a    3.0112    4.7115     0.0333    0.0495   7709.7148    1.0006    ⋯\n           b    3.9132    0.4540     0.0032    0.0051   7151.1739    1.0004    ⋯\n<span class=\"ansi-cyan-fg\">                                                                1 column omitted</span>\nQuantiles\n <span class=\"ansi-bold\"> parameters </span> <span class=\"ansi-bold\">    2.5% </span> <span class=\"ansi-bold\">   25.0% </span> <span class=\"ansi-bold\">   50.0% </span> <span class=\"ansi-bold\">   75.0% </span> <span class=\"ansi-bold\">   97.5% </span>\n <span class=\"ansi-bright-black-fg\">     Symbol </span> <span class=\"ansi-bright-black-fg\"> Float64 </span> <span class=\"ansi-bright-black-fg\"> Float64 </span> <span class=\"ansi-bright-black-fg\"> Float64 </span> <span class=\"ansi-bright-black-fg\"> Float64 </span> <span class=\"ansi-bright-black-fg\"> Float64 </span>\n          σ²    6.0219    7.9104    9.3808   11.3350   17.1375\n           a   -6.5339    0.0383    3.0647    6.0833   12.2396\n           b    3.0253    3.6243    3.9060    4.1948    4.8241\n</pre>\n```\n:::\n\n:::\n:::\n\n\nHow can we interpret the output? The first parts of the summary statistics are straightforward: we get the mean, standard deviation, and Monte Carlo standard error (`mcse`) of each parameter. We also get information about the effective sample size (ESS)^[The ESS reflects the efficiency of the sampler: this is an estimate of the equivalent number of independent samples; the more correlated the samples, the lower the ESS.] and $\\hat{R}$, which measures the ratio of within-chain variance and across-chain variance as a check for convergence^[The closer $\\hat{R}$ is to 1, the better.].\n\nIn this case, we can see that we were able to recover the \"true\" data-generating values of $\\sigma^2 = 10$ and $b = 4$, but $a$ is slightly off (the mean is 3, rather than the data-generating value of 5). This isn't surprising: given the variance of the noise $\\sigma^2$, there are many different intercepts which could fit within that spread.\n\nLet's now plot the chains for visual inspection.\n\n::: {#fig-chains-regression .cell .column-page-inset-right execution_count=7}\n``` {.julia .cell-code}\nplot(chain)\n```\n\n::: {.cell-output .cell-output-display execution_count=76}\n![Output from the MCMC sampler. Each row corresponds to a different parameter: $\\sigma^2$, $a$, and $b$. Each chain is shown in a different color. The left column shows the sampler traceplots, and the right column the resulting posterior distributions.](tutorial_files/figure-ipynb/fig-chains-regression-output-1.png){#fig-chains-regression}\n:::\n:::\n\n\nWe can see from @fig-chains-regression that our chains mixed well and seem to have converged to similar distributions! The traceplots have a \"hairy caterpiller\" appearance, suggesting relatively little autocorrelation. This aligns with the ESS estimates from the `chain` output, which are higher than the number of drawn samples^[Which is possible with Hamiltonian Monte Carlo due to the efficiency of exploration!]. We can also see how much more uncertainty there is with the intercept $a$, while the slope $b$ is much more constrained, with a 95% credible interval of $(3, 4.8)$ (compared to $(-6.5, 12)$ for $a$).\n\nAnother interesting comparison we can make is with the maximum-likelihood estimate (MLE), which we can obtain through optimization.\n\n::: {#e9be12df .cell .column-margin execution_count=8}\n``` {.julia .cell-code}\nmle_model = linear_regression(x, y); # specify the model object\noptimize(mle_model, MLE())\n```\n\n::: {.cell-output .cell-output-display execution_count=77}\n```\nModeResult with maximized lp of -34.57\n3-element Named Vector{Float64}\nA   │ \n────┼────────\n:σ² │ 7.67781\n:a  │ 3.76777\n:b  │ 3.86192\n```\n:::\n:::\n\n\nThe MLE results have some additional error compared to the MCMC means (the slope and noise variance are further from the data-generating values), but this can be a lot faster, at the expense of getting estimates of uncertainty.\n\n### Model Diagnostics and Posterior Predictive Checks\n\nOne advantage of the Bayesian modeling approach here is that we have access to a *generative model*, or a model which we can use to generate datasets. This means that we can now use Monte Carlo simulation, sampling from our posteriors, to look at how uncertainty in the parameter estimates propagates through the model. Let's write a function which gets samples from the MCMC chains and generates datasets.\n\n::: {#6413bb67 .cell execution_count=9}\n``` {.julia .cell-code}\nfunction mc_predict_regression(x, chain)\n    # the Array(group()) syntax is more general than we need, but will work if we have multiple variables which were sampled as a group, for example multiple regression coefficients.\n    a = Array(group(chain, :a))\n    b = Array(group(chain, :b))\n    σ² = Array(group(chain, :σ²))\n\n    μ = a' .+ x * b'\n    y = zeros((length(x), length(a)))\n    for i = 1:length(a)\n        y[:, i] = rand.(Normal.(μ[:, i], σ²[i]))\n    end\n    return y\nend\n```\n\n::: {.cell-output .cell-output-display execution_count=78}\n```\nmc_predict_regression (generic function with 1 method)\n```\n:::\n:::\n\n\nNow we can generate a predictive interval and median and compare to the data.\n\n::: {#1b705e61 .cell execution_count=10}\n``` {.julia .cell-code}\nx_pred = 0:20;\ny_pred = mc_predict_regression(x_pred, chain)\n```\n\n::: {.cell-output .cell-output-display execution_count=79}\n```\n21×20000 Matrix{Float64}:\n   6.68936    -1.81801  -4.58763  …   14.6356    -0.671331   5.57161\n   2.46789     7.18889   9.67166       6.15337   -0.285616  13.6276\n -19.928     -10.5487   15.1344       -8.11842    0.408939  -4.88305\n  33.1545     20.7704   10.2623        6.02563    6.57178    7.53745\n  35.2771     16.3458    8.57409      20.2317    13.3907     4.84027\n  21.1928     26.1562   36.552    …   -0.179761  14.5226    15.3409\n  40.7311     23.572    41.5141       31.8598    37.1869    27.9604\n   0.423933   13.9711   28.51         54.575     32.9363    28.5016\n  62.4813     47.5292   35.4607       37.3601    32.239     30.2444\n  51.1977     27.8843   34.0011       41.9719    52.8179    46.9838\n  45.2684     55.6934   37.2417   …   58.5066    45.3842    26.1518\n  51.8353     38.6125   42.5691       60.4984    50.5223    37.1143\n  56.3781     56.4842   72.109        70.1629    58.2065    56.5514\n  73.8257     41.2734   37.0866       52.9339    41.956     50.9942\n  57.176      47.693    58.0555       88.7714    53.4595    57.0628\n  87.2446     48.9406   47.6912   …   93.4082    72.0358    58.1304\n  60.1674     63.2218   67.6798       88.6353    67.5149    67.8642\n  75.3231     64.1233   62.3664       80.2144    69.696     73.4328\n  76.9033     73.7972   84.0307       82.2377    79.6178    77.3594\n  78.2451     64.1439   78.0576      111.722     69.2794    59.0543\n  76.0979     86.0809   60.8346   …  104.763     91.1638    72.3884\n```\n:::\n:::\n\n\nNotice the dimension of `y_pred`: we have 20,000 columns, because we have 4 chains with 5,000 samples each. If we had wanted to subsample (which might be necessary if we had hundreds of thousands or millions of samples), we could have done that within `mc_linear_regression` before simulation.\n\n::: {#2d954b85 .cell execution_count=11}\n``` {.julia .cell-code}\n# get the boundaries for the 95% prediction interval and the median\ny_ci_low = quantile.(eachrow(y_pred), 0.025);\ny_ci_hi = quantile.(eachrow(y_pred), 0.975);\ny_med = quantile.(eachrow(y_pred), 0.5);\n```\n:::\n\n\nNow, let's plot the prediction interval and median, and compare to the original data.\n\n::: {#fig-prediction-regression .cell execution_count=12}\n``` {.julia .cell-code}\n# plot prediction interval\nplot(x_pred, y_ci_low, fillrange=y_ci_hi, xlabel=L\"$x$\", ylabel=L\"$y$\", fillalpha=0.3, fillcolor=:blue, label=\"95% Prediction Interval\", legend=:topleft, linealpha=0)\n# plot median line\nplot!(x_pred, y_med, color=:blue, label=\"Prediction Median\")\nscatter!(x, y, color=:red, label=\"Data\")\n```\n\n::: {.cell-output .cell-output-display execution_count=81}\n![Posterior 95% predictive interval and median for the linear regression model. The data is plotted in red for comparison.](tutorial_files/figure-ipynb/fig-prediction-regression-output-1.png){#fig-prediction-regression}\n:::\n:::\n\n\nFrom @fig-prediction-regression, it looks like our model might be slightly under-confident, as with 10 data points, we would expect 5% of them (or 1/2 of a \"data point\") to be outside the 95% prediction interval. It's hard to tell with only 10 data points, though! We could resolve this by tightening our priors, but this depends on how much information we used to specify them in the first place. The goal shouldn't be to hit a specific level of uncertainty, but if there is a sound reason to tighten the priors, we could do so.\n\nNow let's look at the residuals from the posterior median and the data. The partial autocorrelations plotted in @fig-residuals-regression are not fully convincing, as there are large autocorrelation coefficients with long lags, but the dataset is quite small, so it's hard to draw strong conclusions. We won't go further down this rabbit hole as we know our data-generating process involved independent noise, but for a real dataset, we might want to try a model specification with autocorrelated data to compare.\n\n::: {#fig-residuals-regression .cell .column-margin execution_count=13}\n``` {.julia .cell-code}\ny_pred_data = mc_predict_regression(x, chain)\ny_med_data = quantile.(eachrow(y_pred_data), 0.5)\nresiduals = y_med_data .- y\nplot(pacf(residuals, 1:4), line=:stem, marker=:circle, legend=:false, grid=:false, linewidth=2, xlabel=\"Lag\", ylabel=\"Partial Autocorrelation\", markersize=8, tickfontsize=14, guidefontsize=16, legendfontsize=16)\nhline!([0], linestyle=:dot, color=:red)\n```\n\n::: {.cell-output .cell-output-display execution_count=82}\n![Partial autocorrelation function of model residuals, relative to the predictive median.](tutorial_files/figure-ipynb/fig-residuals-regression-output-1.png){#fig-residuals-regression}\n:::\n:::\n\n\n## Fitting Extreme Value Models to Tide Gauge Data\n\nLet's now look at an example of fitting an extreme value distribution (namely, a generalized extreme value distribution) to tide gauge data.\n\nFirst, we'll download and plot the data:\n\n::: {#fig-tds-scatter .cell execution_count=14}\n``` {.julia .cell-code}\nfname = \"data/tds.csv\" # CHANGE THIS!\nurl = \"https://www.sciencebase.gov/catalog/file/get/5d5af82de4b01d82ce8ed296?f=__disk__b9%2Fb7%2F40%2Fb9b74037d91c45e730ba4c170fe3ffaf9470d360\"\nif !isfile(fname)\n    Downloads.download(url, fname)\nend\ntds = DataFrame(CSV.File(fname))\ntds[!, [:date, :discharge_cms, :tds_mgL]]\n\np = scatter(tds.discharge_cms, tds.tds_mgL, xlabel=L\"Discharge~($m^3/s$)\", ylabel=L\"Total dissolved solids~($mg/L$)\", title=\"Cuyahoga River, Ohio, 1969–1973\", label=\"Data\")\n```\n\n::: {.cell-output .cell-output-display execution_count=83}\n![Total dissolved solids ($mg/L$) vs. discharge ($m^3/s$) for the Cuyahoga River from 1969 to 1973. Data obtained from [Statistical Methods in Water Resources](https://doi.org/10.3133/tm4A3).](tutorial_files/figure-ipynb/fig-tds-scatter-output-1.png){#fig-tds-scatter}\n:::\n:::\n\n\nNote at @fig-tds-scatter does not look linear at all. However, it does look roughly like an inverse-exponential plot, which suggests that logarithmically-transforming the $x$-axis might look linear.\n\n::: {#fig-tds-log-scatter .cell execution_count=15}\n``` {.julia .cell-code}\nx_ticks = [1, 2, 5, 10, 20, 50, 100]\nplot(p, xaxis=:log, xticks=(x_ticks, string.(x_ticks)))\n```\n\n::: {.cell-output .cell-output-display execution_count=84}\n![The same plot as @fig-tds-scatter, but with the $x$-axis shown on a log scale.](tutorial_files/figure-ipynb/fig-tds-log-scatter-output-1.png){#fig-tds-log-scatter}\n:::\n:::\n\n\n---\njupyter:\n  kernelspec:\n    display_name: Julia 1.8.2\n    language: julia\n    name: julia-1.8\n---\n",
    "supporting": [
      "tutorial_files"
    ],
    "filters": []
  }
}